{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5a4e8-0fc1-4f64-93c7-9ccb1bd481dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import adabelief\n",
    "import talos\n",
    "\n",
    "neural_network='relu_dropout'\n",
    "neural_network_file=f'best_{neural_network}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313ecb8-3684-4194-9c14-fee8668a439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data / 255.0\n",
    "test_data  = test_data / 255.0\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10, dtype='float32')\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10, dtype='float32')\n",
    "\n",
    "train_data = train_data.reshape(train_data.shape[0], 784)\n",
    "test_data  = test_data.reshape(test_data.shape[0], 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a55d07-ff15-4b1e-8a5d-e05c68a4cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEarlyStopping(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 monitor='val_accuracy',\n",
    "                 baseline=0.9,\n",
    "                 baseline_epochs=1,\n",
    "                 restore_best_weights=True):\n",
    "        super(MyEarlyStopping, self).__init__()\n",
    "\n",
    "        self.monitor = monitor\n",
    "        self.baseline = baseline\n",
    "        self.baseline_epochs = baseline_epochs\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_weights = None\n",
    "\n",
    "        if 'acc' in self.monitor:\n",
    "            self.monitor_op = np.greater\n",
    "        else:\n",
    "            self.monitor_op = np.less\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = np.Inf if self.monitor_op == np.less else -np.Inf\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = self.get_monitor_value(logs)\n",
    "        if current is None:\n",
    "            return\n",
    "        if self.restore_best_weights and self.best_weights is None:\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        if self._is_improvement(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = self.model.get_weights()\n",
    "        if self._is_improvement(current, self.baseline):\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        if self.wait >= self.baseline_epochs:\n",
    "            self.stopped_epoch = epoch\n",
    "            self.model.stop_training = True\n",
    "            if self.restore_best_weights and self.best_weights is not None:\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.restore_best_weights and self.best_weights is not None:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "    \n",
    "    def get_monitor_value(self, logs):\n",
    "        logs = logs or {}\n",
    "        monitor_value = logs.get(self.monitor)\n",
    "        if monitor_value is None:\n",
    "            logging.warning('Early stopping conditioned on metric `%s` '\n",
    "                                            'which is not available. Available metrics are: %s',\n",
    "                                            self.monitor, ','.join(list(logs.keys())))\n",
    "        return monitor_value\n",
    "\n",
    "    def _is_improvement(self, monitor_value, reference_value):\n",
    "        return self.monitor_op(monitor_value, reference_value)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "971b5966-b632-43fd-8d28-610493aa756f",
   "metadata": {},
   "source": [
    "Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7682c0d-ddc7-4618-b64e-3c04235815a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'optimizer': [\n",
    "                  lambda lr: adabelief.AdaBeliefOptimizer(lr=lr, amsgrad=True),\n",
    "                  lambda lr: adabelief.AdaBeliefOptimizer(lr=lr, amsgrad=False),\n",
    "                  lambda lr: tf.keras.optimizers.Adam(lr=lr, amsgrad=True),\n",
    "                  lambda lr: tf.keras.optimizers.Adam(lr=lr, amsgrad=False),\n",
    "                  tf.keras.optimizers.Adadelta,\n",
    "                  tf.keras.optimizers.Adagrad,\n",
    "                  tf.keras.optimizers.Adamax,\n",
    "                  tf.keras.optimizers.Nadam,\n",
    "                  lambda lr: tf.keras.optimizers.RMSprop(lr=lr, centered=True),\n",
    "                  lambda lr: tf.keras.optimizers.RMSprop(lr=lr, centered=False),\n",
    "                  lambda lr: tf.keras.optimizers.SGD(lr=lr, nesterov=True),\n",
    "                  lambda lr: tf.keras.optimizers.SGD(lr=lr, nesterov=False),\n",
    "                 ],\n",
    "    'batch_size': [10, 15, 20, 25, 30, 35, 40],\n",
    "    'units': [250],\n",
    "    'lr': [0.001, 0.01, 0.1, 1, 2, 5],\n",
    "    'dropout': [0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "def mnist_model(train_data, train_labels, _, __, params):\n",
    "    model = tf.keras.models.Sequential([\n",
    "                                    tf.keras.layers.Dropout(params['dropout'], input_shape=(784,)),\n",
    "                                    tf.keras.layers.Dense(params['units'], activation='linear'),\n",
    "                                    tf.keras.layers.ReLU(),\n",
    "                                    tf.keras.layers.Dense(params['units'], activation='linear'),\n",
    "                                    tf.keras.layers.ReLU(),\n",
    "                                    tf.keras.layers.Dense(10, activation='linear')\n",
    "                                   ])\n",
    "    optimizer = params['optimizer'](params['lr'])\n",
    "    model.compile(optimizer=optimizer, loss=tf.nn.softmax_cross_entropy_with_logits, metrics=['accuracy'])\n",
    "    early_stopping = MyEarlyStopping(monitor='val_accuracy', baseline=0.6, baseline_epochs=1, restore_best_weights=True)\n",
    "    history = model.fit(train_data, train_labels, validation_data=(test_data, test_labels), batch_size=params['batch_size'], epochs=30, callbacks=[early_stopping], validation_split=0.0, use_multiprocessing=True, verbose=0)    \n",
    "    loss, accuracy = model.evaluate(train_data, train_labels, verbose=0)\n",
    "    val_loss, val_accuracy = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    history.history['loss'], history.history['accuracy'] = [loss], [accuracy]\n",
    "    history.history['val_loss'], history.history['val_accuracy'] = [val_loss], [val_accuracy]\n",
    "    return history, model\n",
    "\n",
    "t = talos.Scan(train_data, train_labels, model=mnist_model, params=p, experiment_name='mnist_experiment', val_split=0., save_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e5c0f-3566-4bd6-a51f-afaffde63b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "file = f\"{neural_network}_file.csv\"\n",
    "\n",
    "df = deepcopy(t.data)\n",
    "df = df.sort_values('val_accuracy', ascending=False)\n",
    "df.to_csv(file)\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "for i, opt in enumerate(p['optimizer'], 1):\n",
    "    df.loc[df.optimizer == str(opt), 'optimizer'] = opt(1)._name + '_' + str(i)\n",
    "\n",
    "df.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8eb1a-a672-4ef0-b920-2861a73f4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9161bbd-38b2-4a67-a7e1-2d2240890c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3d119111-ab02-4395-93ea-f9060e3db3d8",
   "metadata": {},
   "source": [
    "Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4374c5f-d1b7-47eb-911b-d0af430a7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "params_list = [\n",
    "    {'batch_size': 20, 'dropout': 0.3, 'lr': 0.1, 'optimizer': tf.keras.optimizers.Adagrad, 'name': 'Adagrad'},\n",
    "    {'batch_size': 40, 'dropout': 0.3, 'lr': 0.1, 'optimizer': lambda lr: tf.keras.optimizers.SGD(lr=lr, nesterov=False), 'name': 'SGD'},\n",
    "    {'batch_size': 40, 'dropout': 0.3, 'lr': 0.1, 'optimizer': lambda lr: tf.keras.optimizers.SGD(lr=lr, nesterov=True), 'name': 'SGD_nesterov'},\n",
    "    {'batch_size': 30, 'dropout': 0.3, 'lr': 0.001, 'optimizer': lambda lr: adabelief.AdaBeliefOptimizer(lr=lr, amsgrad=True), 'name': 'AdaBelief_amsgrad'},\n",
    "    {'batch_size': 20, 'dropout': 0.2, 'lr': 0.01, 'optimizer': tf.keras.optimizers.Adamax, 'name': 'Adamax'},\n",
    "    {'batch_size': 40, 'dropout': 0.2, 'lr': 2, 'optimizer': tf.keras.optimizers.Adadelta, 'name': 'Adadelta'},\n",
    "    {'batch_size': 25, 'dropout': 0.3, 'lr': 0.001, 'optimizer': lambda lr: tf.keras.optimizers.Adam(lr=lr, amsgrad=True), 'name': 'Adam_amsgrad'},\n",
    "    {'batch_size': 35, 'dropout': 0.2, 'lr': 0.001, 'optimizer': lambda lr: tf.keras.optimizers.Adam(lr=lr, amsgrad=False), 'name': 'Adam'},\n",
    "    {'batch_size': 40, 'dropout': 0.4, 'lr': 0.001, 'optimizer': lambda lr: adabelief.AdaBeliefOptimizer(lr=lr, amsgrad=False), 'name': 'AdaBelief'},\n",
    "    {'batch_size': 40, 'dropout': 0.2, 'lr': 0.001, 'optimizer': tf.keras.optimizers.Nadam, 'name': 'Nadam'},\n",
    "    {'batch_size': 40, 'dropout': 0.2, 'lr': 0.001, 'optimizer': lambda lr: tf.keras.optimizers.RMSprop(lr=lr, centered=False), 'name': 'RMSprop'},\n",
    "    {'batch_size': 40, 'dropout': 0.3, 'lr': 0.001, 'optimizer': lambda lr: tf.keras.optimizers.RMSprop(lr=lr, centered=True), 'name': 'RMSprop_centered'},\n",
    "]\n",
    "\n",
    "def train_model(params):\n",
    "    model = tf.keras.models.Sequential([\n",
    "                                    tf.keras.layers.Dropout(params['dropout'], input_shape=(784,)),\n",
    "                                    tf.keras.layers.Dense(250, activation='linear'),\n",
    "                                    tf.keras.layers.ReLU(),\n",
    "                                    tf.keras.layers.Dense(250, activation='linear'),\n",
    "                                    tf.keras.layers.ReLU(),\n",
    "                                    tf.keras.layers.Dense(10, activation='linear')\n",
    "                                   ])\n",
    "    optimizer = params['optimizer'](params['lr'])\n",
    "    model.compile(optimizer=optimizer, loss=tf.nn.softmax_cross_entropy_with_logits, metrics=['accuracy'])\n",
    "    model.fit(train_data, train_labels, validation_data=(test_data, test_labels), batch_size=params['batch_size'], epochs=50, validation_split=0.0, use_multiprocessing=True, verbose=0)    \n",
    "    loss, accuracy = model.evaluate(train_data, train_labels, verbose=0)\n",
    "    val_loss, val_accuracy = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    return accuracy, val_accuracy, model\n",
    "\n",
    "tests = 10\n",
    "for i, params in enumerate(params_list, 1):\n",
    "    print(f\"{i}/{len(params_list)}. {params['name']}, lr={params['lr']}\")\n",
    "    accuracy_list = []\n",
    "    val_accuracy_list = []\n",
    "    max_accuracy = 0\n",
    "    max_val_accuracy = 0\n",
    "    s = 0\n",
    "    best_model = None\n",
    "    for i in range(1, tests + 1):\n",
    "        start = perf_counter()\n",
    "        accuracy, val_accuracy, model = train_model(params)\n",
    "        stop = perf_counter()\n",
    "        if max_val_accuracy < val_accuracy or max_val_accuracy == val_accuracy and max_accuracy < accuracy:\n",
    "            best_model = model\n",
    "            max_accuracy = accuracy\n",
    "            max_val_accuracy = val_accuracy\n",
    "        s += stop - start\n",
    "        accuracy_list.append(accuracy)\n",
    "        val_accuracy_list.append(val_accuracy)\n",
    "        print(f\"{i}/{tests} | accuracy: {accuracy} | val_accuracy: {val_accuracy} | time: {stop - start} s.\")\n",
    "    s /= tests\n",
    "    if best_model is not None:\n",
    "        loss, accuracy = best_model.evaluate(train_data, train_labels, verbose=0)\n",
    "        val_loss, val_accuracy = best_model.evaluate(test_data, test_labels, verbose=0)\n",
    "        tf.keras.models.save_model(best_model, f\"{neural_network}_{params['name']}_{params['lr']}_{accuracy:g}_{val_accuracy:g}\")\n",
    "    print(f\"accuracy, val_accuracy={list(zip(accuracy_list, val_accuracy_list))}\")\n",
    "    print(f\"mean time={s} s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c643880-fd7d-47b0-abb6-d3dab9391e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "69d60cf4-ec2c-466a-a59b-97062fdec0ba",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5286bc-d1da-4a13-972d-3032c0b6ea64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data.reshape(train_data.shape[0], 784)\n",
    "test_data  = test_data.reshape(test_data.shape[0], 784)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "                                    tf.keras.layers.Dropout(0.4, input_shape=(784,)),\n",
    "                                    tf.keras.layers.Dense(250, activation='linear'),\n",
    "                                    tf.keras.layers.ReLU(),\n",
    "                                    tf.keras.layers.Dense(250, activation='linear'),\n",
    "                                    tf.keras.layers.ReLU(),\n",
    "                                    tf.keras.layers.Dense(10, activation='linear')\n",
    "                                   ])\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adadelta(lr=5),\n",
    "     loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b2260-eca3-4078-b5af-4851ff538ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3ef17a-ede0-4bf4-8346-d69482bdf284",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model, \n",
    "    show_shapes=True, \n",
    "    show_layer_names=True,\n",
    "    expand_nested=True,\n",
    "    dpi = 60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80644280-0376-40b3-9d49-51bb5b69e3b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelcheckpoint_best = tf.keras.callbacks.ModelCheckpoint(neural_network_file, monitor='val_accuracy', save_best_only=True, mode='max', save_weights_only=False)\n",
    "history = model.fit(train_data,train_labels, validation_data=(test_data, test_labels), batch_size=500, epochs=30, validation_split=0.0, use_multiprocessing=True, callbacks=[modelcheckpoint_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925fd56c-a4ec-45b0-9bf2-00b3ba53b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(neural_network_file, custom_objects={'AdaBeliefOptimizer': adabelief.AdaBeliefOptimizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a2118-f5b3-4052-83e5-e273e411cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train evaluate\")\n",
    "evaluation = model.evaluate(train_data, train_labels)\n",
    "print(evaluation)\n",
    "print()\n",
    "print(\"Test evaluate\")\n",
    "evaluation = model.evaluate(test_data, test_labels)\n",
    "print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
